{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tbpny6-szFdW"
   },
   "source": [
    "## **Custom Embeddings**\n",
    "\n",
    "This notebook should be ideally run on google colab. \n",
    "\n",
    "For Google colab, \n",
    "1. Make sure you have added the shared folder \"digital-forest\". \n",
    "2. Mount the google drive onto the colab environment. \n",
    "    1. Go to the folder icon on the left\n",
    "    2. Click on th folder icon with google drive icon.\n",
    "    3. This should mount the drive.\n",
    "    4. Now all files in your drive are directly accessible in your colab environment.\n",
    "\n",
    "For running on local environment, \n",
    "1. Make sure to change the root path to the local directory.\n",
    "2. If any errors make sure to double check the file directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "797LZ0io0gVB"
   },
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAojZyQo0HhN"
   },
   "source": [
    "**Note:-** Here the directory should match the directory from your google colab drive. \n",
    "To get this\n",
    "1. Explore the folders in the files section\n",
    "2. Right Click on the folder whose path you woukld like to import.\n",
    "3. Click on Copy Path from the dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5pLiDf2shAKx"
   },
   "outputs": [],
   "source": [
    "mdpi_dir = '/content/drive/MyDrive/digital-forest/mdpi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tdZ892CLhRy2"
   },
   "outputs": [],
   "source": [
    "# Get a list of all files in given directory\n",
    "from os import walk\n",
    "filenames = next(walk(mdpi_dir), (None, None, []))[2]  # [] if no file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-m0uED0X0nIH",
    "outputId": "cd0346bd-f8e6-4d47-b567-714dbcc25e67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.3390_rs13153009.html',\n",
       " '10.3390_rs13152956.html',\n",
       " '10.3390_rs13152892.html']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk8Vu2-Xh30A"
   },
   "source": [
    "## 2. Get text data from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ur0gyaFeh5KJ"
   },
   "outputs": [],
   "source": [
    "import imp\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_text_from_html(mdpi_dir, mdpi_file_name):\n",
    "    with open(mdpi_dir + '/' + mdpi_file_name, \"r\", encoding='utf-8') as f:\n",
    "        html_file = f.read()\n",
    "    soup = BeautifulSoup(html_file, 'html.parser')\n",
    "    \n",
    "    article = soup.find('article')\n",
    "    text_list = article.find_all(text=True)\n",
    "    article_text = \" \".join(text_list)\n",
    "    \n",
    "    # Remove \\n characters\n",
    "    clean_text = article_text.replace('\\n', ' ')\n",
    "    # Remove special characters and numbers\n",
    "    clean_text = re.sub('[^.,A-Za-z]+', ' ', clean_text)\n",
    "    # Convert all text to lower\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CLEjSsph7xq",
    "outputId": "9f233a16-1862-4428-f8e9-d661a669a4de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while extracting text for 10.3390_rs90201023.html 'NoneType' object has no attribute 'find_all'\n",
      "Error while extracting text for 10.3390_rs90201024.html 'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "# Get all the text data from the articles\n",
    "mdpi_corpus = []\n",
    "failed_files = []\n",
    "for file_name in filenames:\n",
    "    # There might be possible exceptions from extracting text. \n",
    "    # This will catch the exceptions and we can analyze why it failed for some files\n",
    "    try:\n",
    "        extracted_text = extract_text_from_html(mdpi_dir, file_name)\n",
    "        mdpi_corpus.append(extracted_text)\n",
    "    except Exception as e:\n",
    "        failed_files.append(file_name)\n",
    "        print(\"Error while extracting text for {}\".format(file_name), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPCg2AzBiM1P",
    "outputId": "758c64be-88b7-4128-925f-c3e5f717fcf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 402 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Successfully processed {} records\".format(len(mdpi_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8QVy8oi3Rc5"
   },
   "source": [
    "## 3. Setup glove embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVpEbJ233XJ_"
   },
   "source": [
    "**This is required on google colab as data is not stored permenantly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8fQO3_zi5pA",
    "outputId": "3668fc65-60e1-4a31-a821-d4e1a73e235b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-15 15:39:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-04-15 15:39:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 41s  \n",
      "\n",
      "2022-04-15 15:41:46 (5.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMSfPgTDlC3e",
    "outputId": "db865e4d-fd93-4521-f29d-4d429816a126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip \"glove.6B.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEBft_l_4Qxc"
   },
   "source": [
    "## 4. Setup pipeline to make custom embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-r0b14XYjepu"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile, datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Rw4cJwVtjISC"
   },
   "outputs": [],
   "source": [
    "glove_file = datapath('/content/glove.6B.50d.txt')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFqcw7Dg4mrO"
   },
   "source": [
    "### 4.1 Process the corpus to the input format required by Word2Vec algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nyr3Uf8R45uh",
    "outputId": "d2a0bae8-0e23-4fb8-9f39-e9272c0cf14d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "l4c0QPXVme4I"
   },
   "outputs": [],
   "source": [
    "# First we combine all the records into one single string\n",
    "full_text = \" \".join(mdpi_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "YpzMSNMbmVqr"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for document in mdpi_corpus:\n",
    "    # Break down each document in the corpus to list of sentences \n",
    "    sent_list = sent_tokenize(document)\n",
    "    # For each sentence break it into list of words\n",
    "    for sent in sent_list:\n",
    "        word_list = word_tokenize(sent)\n",
    "        sentences.append(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIv8azmQ-RcD",
    "outputId": "f1be7854-526a-4bd4-9106-17d9db62b1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 377179 sentences in the corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} sentences in the corpus\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QK7S53sI_Cth"
   },
   "source": [
    "### 4.2 Setup Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "W1sgZiZGkVy2"
   },
   "outputs": [],
   "source": [
    "# build a word2vec model on your dataset\n",
    "base_model = Word2Vec(size=50, window=5, min_count=3, workers=4)\n",
    "base_model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "WMSbH5b3_V9W"
   },
   "outputs": [],
   "source": [
    "total_examples = base_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRklNiF-_mg_",
    "outputId": "19fa4fa6-04f0-4d6a-e4ab-30baee64544f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26528"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique words in the vocabulary\n",
    "len(base_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIqKacNTAJq-",
    "outputId": "9e97a555-17de-4824-edd1-7cc6996c160d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words to our corpus 6268\n",
      "Common words between corpus and glove 20260\n"
     ]
    }
   ],
   "source": [
    "# Statistics of our vocabulary\n",
    "unique_words = set(base_model.wv.vocab.keys()) - set(glove_vectors.vocab.keys())\n",
    "common_words = set(base_model.wv.vocab.keys()).intersection(set(glove_vectors.vocab.keys()))\n",
    "\n",
    "print(\"Unique words to our corpus {}\".format(len(unique_words)))\n",
    "print(\"Common words between corpus and glove {}\".format(len(common_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwJSsE4i_00l"
   },
   "source": [
    "### 4.3 Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "OsWC-_ldArbK"
   },
   "outputs": [],
   "source": [
    "# update our model with GloVe's vocabulary & weights\n",
    "base_model.build_vocab([list(glove_vectors.vocab.keys())], update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvFz4klH_bxd"
   },
   "outputs": [],
   "source": [
    "# train on your data\n",
    "base_model.train(sentences, total_examples=total_examples, epochs=100)\n",
    "base_model_wv = base_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TejmYH0WAzFy"
   },
   "source": [
    "### 4.4 Analyze our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHzag7fVA4MI",
    "outputId": "6714a6ff-bc5a-46a7-8621-4135cbccd26e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qair',\n",
       " 'k.l',\n",
       " 'singto',\n",
       " 'forclime',\n",
       " 'tection',\n",
       " 'channan',\n",
       " 'logsig',\n",
       " 'mizoue',\n",
       " 'y.o',\n",
       " 'g.o']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VgwmeG0BCjp",
    "outputId": "8ebcf489-84a3-42af-ece6-9c9fb5b38f9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'geoinform' in common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orj3zSoRt5r_",
    "outputId": "6e18f7c4-de0d-409d-ea2e-c09b7bdd5142"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('energy', 0.5695731043815613),\n",
       " ('topography', 0.49855130910873413),\n",
       " ('poulin', 0.4958604872226715),\n",
       " ('levin', 0.4894944727420807),\n",
       " ('sun', 0.48915255069732666),\n",
       " ('vicarious', 0.48822200298309326),\n",
       " ('quantification', 0.48101136088371277),\n",
       " ('gasparini', 0.470840185880661),\n",
       " ('cools', 0.46899086236953735),\n",
       " ('ahola', 0.4679374396800995)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_wv.most_similar('geoinform')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
